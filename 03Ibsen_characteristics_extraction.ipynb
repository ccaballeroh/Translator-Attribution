{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characteristics extraction\n",
    "\n",
    "In this notebook, we'll extract characteristics from the pre-processed corpus. The pre-processing consisted on removing numbers between brackets and parentheses and special characters. The characteristics are unigrams, bigrams, and trigrams of words and of POS (considering and ignoring punctuation).\n",
    "\n",
    "In order to obtain these characteristics, we need to tokenize and to tag the texts, but not to parse them. However, we're going to use the entire pipeline and save the result to disk so we can use it later for extracting syntactic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones propias\n",
    "from helper.functions import proc_texts, get_translator, save_dataset_to_json\n",
    "from helper.features import extract_features_unigrams, extract_features_bigrams, extract_features_trigrams\n",
    "from helper.features import extract_features_bigramsPOS, extract_features_trigramsPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_FOLDER = Path(r\"./Corpora/Proc_Ibsen_final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [(proc_texts(file, nlp), file.name) for file in CORPUS_FOLDER.iterdir() if file.suffix == \".txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the result to disk using the pickle protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "doc_data = pickle.dumps(docs)\n",
    "\n",
    "with open(\".\\\\auxfiles\\pickle\\\\ibsen_proc.pickle\", \"wb\") as f:\n",
    "    f.write(doc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to pick up the process from this point, we can load the pickle file from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\".\\\\auxfiles\\pickle\\\\ibsen_proc.pickle\", \"rb\") as f:\n",
    "#     doc_data=f.read()\n",
    "\n",
    "# docs = pickle.loads(doc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word n-gram extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following two cells we extract unigrams, bigrams, trigrams for the entire corpus disregarding punctuation, and save to disk the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset_unigrams = [(extract_features_unigrams(doc, punct=False), get_translator(filename)) for doc, filename in docs]\n",
    "featureset_bigrams = [(extract_features_bigrams(doc, punct=False), get_translator(filename)) for doc, filename in docs]\n",
    "featureset_trigrams = [(extract_features_trigrams(doc, punct=False), get_translator(filename)) for doc, filename in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved\n",
      "file saved\n",
      "file saved\n"
     ]
    }
   ],
   "source": [
    "save_dataset_to_json(featureset_unigrams, \"featuresIbsen_unigrams\")\n",
    "save_dataset_to_json(featureset_bigrams, \"featuresIbsen_bigrams\")\n",
    "save_dataset_to_json(featureset_trigrams, \"featuresIbsen_trigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following two cells we repeat the process, but taking into account the punctuation this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset_unigrams_punct = [(extract_features_unigrams(doc, punct=True), get_translator(filename)) for doc, filename in docs]\n",
    "featureset_bigrams_punct = [(extract_features_bigrams(doc, punct=True), get_translator(filename)) for doc, filename in docs]\n",
    "featureset_trigrams_punct = [(extract_features_trigrams(doc, punct=True), get_translator(filename)) for doc, filename in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved\n",
      "file saved\n",
      "file saved\n"
     ]
    }
   ],
   "source": [
    "save_dataset_to_json(featureset_unigrams_punct, \"featuresIbsen_unigrams_punct\")\n",
    "save_dataset_to_json(featureset_bigrams_punct, \"featuresIbsen_bigrams_punct\")\n",
    "save_dataset_to_json(featureset_trigrams_punct, \"featuresIbsen_trigrams_punct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS n-gram extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll repeat the process but for POS bigrams and trigrams. Again, taking the punctuation into account and not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset_bigrams_pos = [(extract_features_bigramsPOS(doc, punct=False), get_translator(filename)) for doc, filename in docs]\n",
    "featureset_trigrams_pos = [(extract_features_trigramsPOS(doc, punct=False), get_translator(filename)) for doc, filename in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved\n",
      "file saved\n"
     ]
    }
   ],
   "source": [
    "save_dataset_to_json(featureset_bigrams_pos, \"featuresIbsen_bigrams_pos\")\n",
    "save_dataset_to_json(featureset_trigrams_pos, \"featuresIbsen_trigrams_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset_bigrams_pos_punct = [(extract_features_bigramsPOS(doc, punct=True), get_translator(filename)) for doc, filename in docs]\n",
    "featureset_trigrams_pos_punct = [(extract_features_trigramsPOS(doc, punct=True), get_translator(filename)) for doc, filename in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved\n",
      "file saved\n"
     ]
    }
   ],
   "source": [
    "save_dataset_to_json(featureset_bigrams_pos_punct, \"featuresIbsen_bigrams_pos_punct\")\n",
    "save_dataset_to_json(featureset_trigrams_pos_punct, \"featuresIbsen_trigrams_pos_punct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells with leftover code that might come handy in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create set of vocab of words\n",
    "# def create_vocab_words(docs):\n",
    "#     vocab_words = set()\n",
    "#     for doc,_ in docs:\n",
    "#         words = ngrams_tokens(doc, n=1)\n",
    "#         strings_words = [token.text.lower() for token, in words]\n",
    "#         vocab_words = vocab_words.union(set(strings_words))\n",
    "#     return vocab_words\n",
    "\n",
    "# # Create set of vocab of bigrams\n",
    "# def create_vocab_bigrams(docs):\n",
    "#     vocab_bigrams = set()\n",
    "#     for doc,_ in docs:\n",
    "#         bigrams = ngrams_tokens(doc, n=2)\n",
    "#         strings_bigrams = [\" \".join([token1.text.lower(), token2.text.lower()])\n",
    "#                             for token1, token2 in bigrams]\n",
    "#         vocab_bigrams = vocab_bigrams.union(set(strings_bigrams))\n",
    "#     return vocab_bigrams\n",
    "\n",
    "# # Create set of vocab of trigrams\n",
    "# def create_vocab_trigrams(docs):\n",
    "#     vocab_trigrams = set()\n",
    "#     for doc,_ in docs:\n",
    "#         trigrams = ngrams_tokens(doc, n=3)\n",
    "#         strings_trigrams = [\" \".join([token1.text.lower(), token2.text.lower(), token3.text.lower()])\n",
    "#                             for token1, token2, token3 in trigrams]\n",
    "#         vocab_trigrams = vocab_trigrams.union(set(strings_trigrams))\n",
    "#     return vocab_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = create_vocab_words(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spacy]",
   "language": "python",
   "name": "conda-env-spacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
