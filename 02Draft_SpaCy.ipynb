{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus files are located in the *Raw_Corpus* folder. Those were downloaded from the Project Gutenberg webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "raw_corpus_folder = \".\\\\Corpora\\\\Raw_Ibsen\\\\\"\n",
    "proc_corpus_folder = \".\\\\Corpora\\\\Proc_Ibsen\\\\\"\n",
    "\n",
    "files_list = os.listdir(raw_corpus_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_front_back_matter(input_folder, filename, output_folder):\n",
    "    \"\"\"Remove legal information from Project Gutenberg files.\n",
    "    \n",
    "    Reads the file with 'filename' in the 'input_folder' folder and\n",
    "    outputs the same file with the \"proc\" word appended at the end\n",
    "    of the filename in the 'output_folder', but without the lines at\n",
    "    the beginning and at the end of the original file containing\n",
    "    legal information from Project Gutenberg.\n",
    "    \n",
    "    :input_folder 'String' - name of the input folder\n",
    "    :filename     'String' - name of the file to process\n",
    "    :out_folder   'String' - name of the outout folder\n",
    "    \n",
    "    It returns None\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = []\n",
    "    write = False\n",
    "    with open(input_folder + filename, \"r\", encoding=\"UTF-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"*** START OF\"):\n",
    "                write = True\n",
    "            elif line.strip().startswith(\"*** END OF\"):\n",
    "                write = False\n",
    "                break\n",
    "            else:\n",
    "                if write:\n",
    "                    lines.append(line)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    with open(\"\".join([output_folder, filename[:-4], \"_proc.txt\"]), \"a\", encoding=\"UTF-8\") as g:\n",
    "        for line in lines:\n",
    "            g.write(line)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the front and back matter for each file in the *Raw_Corpus* folder. We place the outputs on the *proc_corpus_folder*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_list:\n",
    "    remove_front_back_matter(raw_corpus_folder, file, proc_corpus_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(input_folder, filename, CHUNK_SIZE=5000):\n",
    "    \"\"\"Generator that yields the following chunk of the file.\n",
    "    \n",
    "    The output is a string with the following chunk size\n",
    "    CHUNK_SIZE of the file 'filename' in the folder 'input folder'.\n",
    "    \n",
    "    :input_folder  'String' - name of input folder\n",
    "    :filename      'String' - name of file to process\n",
    "    :CHUNK_SIZE    'Integer' - size of chunk\n",
    "    \n",
    "    yields a 'String' of size of 'CHUNK_SIZE'\n",
    "    \"\"\"\n",
    "    SIZE = os.stat(input_folder + filename).st_size  # filesize\n",
    "    with open(input_folder + filename, \"r\", encoding=\"UTF-8\") as f:\n",
    "        for _ in range(SIZE//CHUNK_SIZE):\n",
    "            # reads the lines that amount to the Chunksize\n",
    "            # and yields a string \n",
    "            yield \"\".join(f.readlines(CHUNK_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_signos(strTexto):\n",
    "    \"\"\"Retira signos no alfanuméricos y números de un texto.\n",
    "\n",
    "    Los reemplaza con un espacio y regresa una lista con cada palabra.\n",
    "\n",
    "    Parámetros\n",
    "    --------------\n",
    "    strTexto : cadena\n",
    "                Texto donde reemplazar los caracteres no alfanuméricos.\n",
    "\n",
    "    Regresa\n",
    "    --------------\n",
    "    lstTokens : lista\n",
    "                cada elemento es una palabra del texto.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # patrón de uno o más caracteres no alfanuméricos o números\n",
    "    reNoAlfanum = re.compile(r\"([^\\w]+|[\\d]+)\")\n",
    "    # sustitución por un espacio\n",
    "    strTextoNuevo = reNoAlfanum.sub(r\" \", strTexto)\n",
    "    # división del texto por espacio en blanco\n",
    "    lstTokens = strTextoNuevo.split()\n",
    "    return lstTokens\n",
    "\n",
    "def calcula_ngramas(lstTokens, n):\n",
    "    \"\"\"Calcula n-gramas de una lista de tokens.\n",
    "\n",
    "    Regresa una lista con los n-gramas.\n",
    "\n",
    "    Parámetros\n",
    "    -------------\n",
    "    lstTokens : lista\n",
    "                Cada elemento de es un token.\n",
    "    n : entero\n",
    "        tamaño del n-grama\n",
    "\n",
    "    Regresa\n",
    "    -------------\n",
    "    ngramas : lista\n",
    "              Cada elemento es un n-grama.\"\"\"\n",
    "    ngramas = []\n",
    "    for i in range(len(lstTokens)+1-n):\n",
    "        ngrama = []\n",
    "        for j in range(i, i+n):\n",
    "            ngrama.append(lstTokens[j])\n",
    "        ngramas.append(\" \".join(ngrama))\n",
    "    return ngramas\n",
    "\n",
    "def frecuencias_ngramas(strTexto, n, tokenizer=remover_signos):\n",
    "    \"\"\"Número de ocurrencias de cada n-grama de un texto.\n",
    "\n",
    "    Retira los signos de puntuación y símbolos. Además no distingue\n",
    "    mayúscula o minúscula.\n",
    "\n",
    "    Parámetros\n",
    "    ------------------\n",
    "    strTexto : cadena\n",
    "                Texto donde hacer el conteo.\n",
    "\n",
    "    n : tamaño del ngrama\n",
    "\n",
    "    Regresa\n",
    "    -----------------\n",
    "    dicFrecuencias : diccionario\n",
    "    las llaves del diccionario son los ngramas, y los valores,\n",
    "    el número de ocurrencias de cada ngrama respectivo.\n",
    "    \"\"\"\n",
    "    dicFrecuencias = {}  # diccionario donde se guardará la salida\n",
    "\n",
    "    lstTokens = tokenizer(strTexto)  # se remueven signos del texto\n",
    "    ngramas = calcula_ngramas(lstTokens, n)  # lista con ngramas\n",
    "\n",
    "    for elemento in ngramas:\n",
    "        # si el ngrama no está en el diccionario ya, se agrega con valor de 1.\n",
    "        # en caso contrario, se le suma un 1 al número de ocurrencias.\n",
    "        if elemento.lower() not in dicFrecuencias:\n",
    "            dicFrecuencias[elemento.lower()] = 1\n",
    "        else:\n",
    "            dicFrecuencias[elemento.lower()] += 1\n",
    "    return dicFrecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, n):\n",
    "    \"\"\"Extract features from a text string.\n",
    "    \n",
    "    :text  'String' - contains the text from where to extract features\n",
    "    \"\"\"\n",
    "    f = frecuencias_ngramas(text, n, tokenizer=tokenize.word_tokenize)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translator(filename):\n",
    "    \"\"\"Get name of translator from file name.\n",
    "    \n",
    "    The filename must have the convention: '{Translator}_{Work}[_proc].txt'\n",
    "    \n",
    "    :filename  'String' - Name of file from which to fetch the translator name\n",
    "    \n",
    "    Returns a 'String' with the translator name.\n",
    "    \"\"\"\n",
    "    return filename.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(proc_corpus_folder, word=\"proc\", n=1):\n",
    "    \"\"\"Create dataset from collections of files with a word in common in the name.\n",
    "    \n",
    "    Creates a dataset from the files contained in the folder 'proc_corpus_folder'.\n",
    "    The dataset is a list of tuples, where the first element of the tuple is features\n",
    "    extracted from the text and the second element is the name of the translator.\n",
    "    \n",
    "    :proc_corpus_folder  'String' - folder containing the files to analyze.\n",
    "    :word                'String' - word that the files must have in common\n",
    "                                    \"proc\" is a default common word of all\n",
    "                                    processed text files.\n",
    "    \n",
    "    Returns a list of the form [({features}, translator, ...]\"\"\"\n",
    "    proc_files_list = os.listdir(proc_corpus_folder)\n",
    "    dataset = []\n",
    "    for file in (filename for filename in proc_files_list if word in filename):\n",
    "        generator = chunks(proc_corpus_folder, file)\n",
    "        dataset.extend([(extract_features(text, n), get_translator(file)) for text in generator])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_files_list = os.listdir(proc_corpus_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = proc_files_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = chunks(proc_corpus_folder, file, CHUNK_SIZE=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This passage is interesting as showing clearly the point of view from\n",
      "which Ibsen conceived the character of Manders. In the next paragraph\n",
      "of the same letter he discusses the attitude of \"the so-called Liberal\n",
      "press\"; but as the paragraph contains the germ of _An Enemy of the\n",
      "People_, it may most fittingly be quoted in the introduction to that\n",
      "play.\n",
      "\n",
      "Three days later (January 6) Ibsen wrote to Schandorph, the Danish\n",
      "novelist: \"I was quite prepared for the hubbub. If certain of our\n",
      "Scandinavian reviewers have no talent for anything else, they have\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = generator.__next__()\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\\\auxfiles\\\\csv\\\\prueba.csv\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    print(\"\\t\".join(\"TEXT POS LEMMA TAG DEP\".split()), file=f)\n",
    "    for token in doc:\n",
    "        print(\"\\t\".join([token.text, token.pos_, token.lemma_, token.tag_, token.dep_]), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Mary loves John, but she does not love him back.\")\n",
    "with open(\".\\\\auxfiles\\\\json\\\\prueba.json\", \"w\") as f:\n",
    "    print(str(doc.to_json()).replace(\"'\", '\"'), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spacy]",
   "language": "python",
   "name": "conda-env-spacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
