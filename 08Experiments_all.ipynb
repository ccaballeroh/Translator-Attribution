{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.functions import get_dataset_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = Path(r\"./auxfiles/json/\")\n",
    "LOG_PATH = Path(r\"./auxfiles/logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_files = [file for file in FOLDER.iterdir() if file.name.startswith(\"features\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes_Ibsen = [\" \".join(file[:-5].split(\"_\")[1:]) for file in features_files if file.split(\"_\")[0].endswith(\"Ibsen\")]\n",
    "# indexes_Quixote = [\" \".join(file[:-5].split(\"_\")[1:]) for file in features_files if file.split(\"_\")[0].endswith(\"Quixote\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all_corpora = defaultdict(pd.DataFrame)\n",
    "\n",
    "for translator in [\"Quixote\",\"Ibsen\"]:\n",
    "    indexes = []\n",
    "    cols = [\"SVC\", \"Random Forest\", \"Logistic Regression\"]\n",
    "    results = []\n",
    "    for file in [file for file in features_files if file.name.split(\"_\")[0].endswith(translator)]:\n",
    "        with open(LOG_PATH/\"experiment_standard.log\", \"a\") as f:\n",
    "            print(file, file=f)\n",
    "        X_dict, y_str = get_dataset_from_json(file)\n",
    "        v = DictVectorizer(sparse=True)\n",
    "        encoder = LabelEncoder()\n",
    "\n",
    "        X = v.fit_transform(X_dict, )\n",
    "        y = encoder.fit_transform(y_str)\n",
    "\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        try:\n",
    "            svm_model = Pipeline([(\"scaler\", StandardScaler(with_mean=False)),\n",
    "                                  (\"scv\", LinearSVC(random_state=42)),\n",
    "                                 ])\n",
    "            cv_svm = cross_val_score(svm_model, X, y, cv=kf, n_jobs=-1)\n",
    "\n",
    "            #nb_model = GaussianNB()\n",
    "            #cv_nb = cross_val_score(nb_model, X.toarray(), y, cv=kf)\n",
    "\n",
    "            rf_model = RandomForestClassifier(n_estimators=100)\n",
    "            cv_rf = cross_val_score(rf_model, X, y, cv=kf, n_jobs=-1)\n",
    "\n",
    "            log_model = LogisticRegression()\n",
    "            cv_log = cross_val_score(log_model, X, y, cv=kf, n_jobs=-1)\n",
    "        \n",
    "        except MemoryError:\n",
    "            cv_svm = -1*np.ones((1,4))\n",
    "            #cv_nb  = cv_svm\n",
    "            cv_rf = cv_svm\n",
    "            cv_log = cv_svm\n",
    "        \n",
    "        result_per_featureset = [cv_svm.mean(), cv_rf.mean(), cv_log.mean()]\n",
    "        #print(result_per_featureset)\n",
    "        \n",
    "        results.append(result_per_featureset)        \n",
    "        indexes.append(\" \".join(file[:-5].split(\"_\")[1:]))\n",
    "    \n",
    "    #print(results)\n",
    "    results_all_corpora[translator] = pd.DataFrame(np.array(results), index=indexes, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = dict(results_all_corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to CSV, $\\LaTeX$, and HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = Path(r\"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for translator in [\"Quixote\", \"Ibsen\"]:\n",
    "    df = final[translator].sort_index()\n",
    "    \n",
    "    df.to_csv(f\"{RESULTS_FOLDER/(translator+'_scaled.csv')}\", float_format='%.4f')\n",
    "    \n",
    "    latex = df.to_latex(float_format=lambda x: '%.4f' % x)\n",
    "    with open(RESULTS_FOLDER/(translator+\"_scaled.tex\"), \"w\") as f:\n",
    "        f.write(latex)\n",
    "    \n",
    "    html = df.to_html(float_format='%.4f')\n",
    "    with open(RESULTS_FOLDER/(translator+\"_scaled.html\"), \"w\") as f:\n",
    "        f.write(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spacy]",
   "language": "python",
   "name": "conda-env-spacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
